# Transformer Hands-On

Hands-on coding exercises and projects to practice **Transformers, LLMs, and Generative AI**.  

---

## ðŸš€ Goals
- Understand the math behind **attention** and **multi-head attention**.
- Implement **Transformer blocks** from scratch.
- Fine-tune **pretrained LLMs** (GPT-2, LLaMA, etc.).
- Explore **GenAI applications** like chatbots and RAG pipelines.
- Practice **efficiency techniques** (KV cache, quantization, RoPE).

---

## ðŸ“‚ Structure
01_attention_numpy/ # Scaled dot-product attention 
02_multihead_attention/ # Multi-head attention 
03_transformer_block/ # Encoder/decoder block 
04_huggingface_finetune/ # GPT-2 fine-tuning 
05_rag_pipeline/ # Retrieval-Augmented Generation 
06_efficiency/ # KV cache, quantization


---

## ðŸ§© Exercises
- **Stage 1:** Implement attention in NumPy.
- **Stage 2:** Build a mini Transformer in PyTorch.
- **Stage 3:** Fine-tune GPT-2 with Hugging Face.
- **Stage 4:** Create a chatbot and RAG pipeline.
- **Stage 5:** Optimize with KV cache and quantization.

---

## âœ… How to Use
Clone the repo and run exercises step by step:

```bash
git clone https://github.com/MarziehVahdat/transformer-hands-on
cd transformer-hands-on

---

## ðŸ“œ License
This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.
